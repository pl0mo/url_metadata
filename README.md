A cache which saves URL metadata and content

This is meant to provide more context to any of my tools which use URLs. If I [watched some youtube video](https://github.com/seanbreckenridge/mpv-sockets/blob/master/DAEMON.md) and I have a URL, I'd like to have the subtitles for it, so I can do a text-search over all the videos I've watched. If I [read an article](https://github.com/seanbreckenridge/ffexport), I want the article text! This requests, parses and abstracts away that data for me locally, so I can just do:

```python
>>> from url_metadata import metadata
>>> m = metadata("https://pypi.org/project/beautifulsoup4/")
>>> len(m.info["images"])
46
>>> m.info["title"]
'beautifulsoup4'
>>> print(m.text_summary)
Beautiful Soup is a library that makes it easy to scrape information
from web pages. It sits atop an HTML or XML parser, providing Pythonic
idioms for iterating, searching, and modifying the parse tree.
....
```

If I ever request the same URL again, that info is grabbed from a local directory cache instead.

---

## Installation

Requires `python3.7+`

To install with pip, run:

    pip install git+https://github.com/seanbreckenridge/url_metadata

---

This uses:

- [`lassie`](https://github.com/michaelhelmick/lassie) to get generic metadata, like the title, description, opengraph information, links to images/videos on the page
- [`readability`](https://github.com/buriy/python-readability) to convert HTML to readable HTML content.
- [`bs4`](https://pypi.org/project/beautifulsoup4/) to convert the parsed HTML to text (to allow for nicer text searching)
- [`youtube_subtitles_downloader`](https://github.com/seanbreckenridge/youtube_subtitles_downloader) to get manual/autogenerated captions (converted to a `.srt` file) from youtube URLs

---

Other planned things:

- CLI interface to cache URLs, extract filenames, types of files; provide an interface to integrate with other shell tools.
- allow user to configure subtitle language, loglevel, etc.
- add examples for library/shell usage
- other data providers?

The core functionality of requesting/parsing/caching URLs is complete.

---

Usage:

TODO: add python library usage

---

Searching:

TODO: add CLI/external usage

---

This stores all of this information as individual files in a cache directory (using [`appdirs`](https://github.com/ActiveState/appdirs)). In particular, it `MD5` hashes the URL and stores information like:

```
.
└── 7
    └── b
        └── 0
            └── d952fd7265e8e4cf10b351b6f8932
                └── 000
                    ├── epoch_timestamp.txt
                    ├── key
                    ├── metadata.json
                    ├── subtitles.srt
                    ├── summary.html
                    └── summary.txt
```

You're free to delete any of the directories in the cache if you want, this doesn't maintain a strict index, it uses a hash of the URL and then searches for a matching `key` file. See comments [here](https://github.com/seanbreckenridge/url_metadata/blob/master/src/url_metadata/cache.py) for implementation details.

By default this waits 5 seconds between requests. Since all the info is cached, I use this by requesting all the info from one data source (e.g. my bookmarks, or videos I've watched recently) in a loop in the background, which saves all the information to my computer. The next time I do that same loop, it doesn't have to make any requests and it just grabs all the info from local cache.

